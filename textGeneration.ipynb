{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referred \n",
    "https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb#scrollTo=ovpZyIhNIgoq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Added by Tomb \n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Added by Tomb\n",
    "path_to_file = pathlib.Path('/Volumes/MAGIC1/CS50/myMusicGen/data/shakespeare.txt')\n",
    "if not path_to_file.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'shakespeare.txt', # your desired file name, you can even name it tomato.mid but the convention is to get the name from the end of the URL path\n",
    "      origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read then decode for py2 compat (python 2 compatibility)\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3'), (10, ':'), (11, ';'), (12, '?'), (13, 'A'), (14, 'B'), (15, 'C'), (16, 'D'), (17, 'E'), (18, 'F'), (19, 'G'), (20, 'H'), (21, 'I'), (22, 'J'), (23, 'K'), (24, 'L'), (25, 'M'), (26, 'N'), (27, 'O'), (28, 'P'), (29, 'Q'), (30, 'R'), (31, 'S'), (32, 'T'), (33, 'U'), (34, 'V'), (35, 'W'), (36, 'X'), (37, 'Y'), (38, 'Z'), (39, 'a'), (40, 'b'), (41, 'c'), (42, 'd'), (43, 'e'), (44, 'f'), (45, 'g'), (46, 'h'), (47, 'i'), (48, 'j'), (49, 'k'), (50, 'l'), (51, 'm'), (52, 'n'), (53, 'o'), (54, 'p'), (55, 'q'), (56, 'r'), (57, 's'), (58, 't'), (59, 'u'), (60, 'v'), (61, 'w'), (62, 'x'), (63, 'y'), (64, 'z')]\n",
      "65 Unique characters\n"
     ]
    }
   ],
   "source": [
    "# the unique characters in the file, meaning each character that is used \n",
    "vocab = sorted(set(text)) # return as a list\n",
    "print ([text for text in enumerate(vocab)])\n",
    "print(f'{len(vocab)} Unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It needs the text to be split into tokens first.\n",
    "# the 'b' prefix printed out indicates that these characters are stored as bytes objects.\n",
    "# When text data is read or processed in Python, especially with encoding methods like UTF-8, each character is represented as a byte in memory\n",
    "\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8') # contains a tensor where each element corresponds to a token (character) from the input texts provided in the example_texts list.\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create the tf.keras.layers.StringLookup layer:\n",
    "# This is NOT zero index. It starts from 1. and the IDs given are unique indices from the vocab variable. \"a\"'s index is 39 in vaocab as zero index\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary = list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invert back the vectors of IDs to Chars\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1) # chars_from_ids is an instance obj of tf.keras.layers.StringLookup, Joins all strings into a single string, or joins along an axis.\n",
    "\n",
    "# To make sure if it works\n",
    "# x = text_from_ids(ids)\n",
    "# print(\"x!!\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Examples and Targets\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs!!>>> tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First prepare IDs, then use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices.\"\"\"\n",
    "# tf.strings.unicode_split makes each chars in text tokens\n",
    "# ids_from_chars variable has a class of tf.keras.layers.StringLookup in it which converts tokens into ids\n",
    "# print(\"tokens!!>>>\", tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(\"IDs!!>>>\",all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # ids_dataset becomes a class\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
    "\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convert these individual characters to sequences of the desired size\"\"\"\n",
    "# The batch method lets you easily do that.\n",
    "#  seq_length + 1, the model learns to predict the next character in the sequence given a fixed window of characters as input.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# It's easier to see what this is doing if you join the tokens back into strings:\n",
    "for seq in sequences.take(1):\n",
    "    print(text_from_ids(seq).numpy()) # text_from_ids is the custom function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. <br> At each time step the input is the current character and the label is the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Details below, but this function slides one char to right\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) #  map automatically takes an argument. dataset is an instance obj of the _MapDataset class\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches\n",
    "You used **tf.data** to split the text into manageable sequences. <br>\n",
    "But before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[51, 44,  2, ..., 62,  2, 53],\n",
      "       [48, 54, 53, ..., 59, 47, 44],\n",
      "       [53,  2, 64, ...,  2, 32, 54],\n",
      "       ...,\n",
      "       [54, 57, 58, ..., 12,  1, 36],\n",
      "       [48, 42, 48, ...,  2, 44, 40],\n",
      "       [64, 44, 59, ...,  2, 47, 48]]), array([[44,  2, 64, ...,  2, 53, 54],\n",
      "       [54, 53,  1, ..., 47, 44, 57],\n",
      "       [ 2, 64, 54, ..., 32, 54, 51],\n",
      "       ...,\n",
      "       [57, 58, 47, ...,  1, 36, 47],\n",
      "       [42, 48, 54, ..., 44, 40, 42],\n",
      "       [44, 59,  2, ..., 47, 48, 52]]))]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) # This does Asynchronous data loading, i.e. improves the loading speed\n",
    ")\n",
    "# dataset\n",
    "print(list(dataset.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '[UNK]'), (1, '\\n'), (2, ' '), (3, '!'), (4, '$'), (5, '&'), (6, \"'\"), (7, ','), (8, '-'), (9, '.'), (10, '3'), (11, ':'), (12, ';'), (13, '?'), (14, 'A'), (15, 'B'), (16, 'C'), (17, 'D'), (18, 'E'), (19, 'F'), (20, 'G'), (21, 'H'), (22, 'I'), (23, 'J'), (24, 'K'), (25, 'L'), (26, 'M'), (27, 'N'), (28, 'O'), (29, 'P'), (30, 'Q'), (31, 'R'), (32, 'S'), (33, 'T'), (34, 'U'), (35, 'V'), (36, 'W'), (37, 'X'), (38, 'Y'), (39, 'Z'), (40, 'a'), (41, 'b'), (42, 'c'), (43, 'd'), (44, 'e'), (45, 'f'), (46, 'g'), (47, 'h'), (48, 'i'), (49, 'j'), (50, 'k'), (51, 'l'), (52, 'm'), (53, 'n'), (54, 'o'), (55, 'p'), (56, 'q'), (57, 'r'), (58, 's'), (59, 't'), (60, 'u'), (61, 'v'), (62, 'w'), (63, 'x'), (64, 'y'), (65, 'z')]\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in StringLookup layer\n",
    "vocab_size =len(ids_from_chars.get_vocabulary())\n",
    "print([x for x in enumerate(ids_from_chars.get_vocabulary())])\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This model has 3 layers <br>\n",
    "-tf.keras.layers.Embedding: The input layer. <br>\n",
    "-tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an **LSTM** layer here.) <br>\n",
    "-tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. <br>\n",
    "\n",
    "Make those as instance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    # The call method defines the forward pass of the model.\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training) \n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instant object\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Model\n",
    "Run the model to see that it behaves as expected.<br>\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, seq_length, vocab_size)\n",
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): # the len of input and target is both 64\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, seq_length, vocab_size)\")\n",
    "    \n",
    "    # the UNK token is added on vocab_size =len(ids_from_chars.get_vocabulary()) if you use Lookup layer, the size will automatically differ fron the original vocab (65) to vocab_size (66) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\"\"\"\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # 2D tensor\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # 1D tensor\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Unused or test code below---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About tf.keras.layersStringLookup()'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About tf.keras.layersStringLookup()\"\"\"\n",
    "\n",
    "# vocab1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# data = tf.constant([[\"a\", \"c\", \"d\"],[\"d\", \"z\", \"b\"]])\n",
    "# print(data)\n",
    "# layer = tf.keras.layers.StringLookup(vocabulary=vocab1)\n",
    "# layer(data)\n",
    "\n",
    "# vocab = ['apple', 'banana', 'orange']\n",
    "# string_lookup = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token='UNK') # UNK refers unknown and it gives the unknown element the index 0. StringLookup starts at 1 index\n",
    "\n",
    "# ids = string_lookup(['apple', 'kiwi', 'orange'])\n",
    "# print(ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about batch in tf.data.Dataset'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"about batch in tf.data.Dataset\"\"\"\n",
    "# dataset = tf.data.Dataset.range(8)\n",
    "# dataset = dataset.batch(3)\n",
    "# print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "\n",
    "# dataset1 = tf.data.Dataset.range(8)\n",
    "# dataset1 = dataset1.batch(3, drop_remainder=True)\n",
    "# print(list(dataset1.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About take() in tf.data.Dataset'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About take() in tf.data.Dataset\"\"\"\n",
    "# dataset = tf.data.Dataset.range(10)\n",
    "# dataset = dataset.take(1)\n",
    "# list(dataset.as_numpy_iterator())\n",
    "\n",
    "# list(sequences.as_numpy_iterator())\n",
    "\n",
    "# seq = sequences.take(1)\n",
    "# list(seq.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l']\n",
      "['e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"How the split_input_target function works\"\"\"\n",
    "\n",
    "# sequence = [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "# sequence = list(\"Hello\") # equivalent above but more convenient\n",
    "\n",
    "# input, target = split_input_target(sequence)\n",
    "# print(input)\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'abcdef' b'123456']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"About reduce_join\"\"\"\n",
    "# join = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']]).numpy()\n",
    "\n",
    "# join1 = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']], axis=-2).numpy() # axis=0 or -2 is column, axis=1 or -1 is row\n",
    "\n",
    "# join2 = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']],\n",
    "#                        axis=-1,\n",
    "#                        separator=\"<<<Tomb>>>\").numpy()\n",
    "\n",
    "# print(join)\n",
    "# print(join1)\n",
    "# print(join2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
