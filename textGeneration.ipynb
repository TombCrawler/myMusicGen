{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referred \n",
    "https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb#scrollTo=ovpZyIhNIgoq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Added by Tomb \n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "#Added by Tomb\n",
    "path_to_file = pathlib.Path('/Volumes/MAGIC1/CS50/myMusicGen/data/shakespeare.txt')\n",
    "if not path_to_file.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'shakespeare.txt', # your desired file name, you can even name it tomato.mid but the convention is to get the name from the end of the URL path\n",
    "      origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )\n",
    "\n",
    "# path_to_file = pathlib.Path('/Volumes/MAGIC1/CS50/myMusicGen/data/MJLyrics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read then decode for py2 compat (python 2 compatibility)\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3'), (10, ':'), (11, ';'), (12, '?'), (13, 'A'), (14, 'B'), (15, 'C'), (16, 'D'), (17, 'E'), (18, 'F'), (19, 'G'), (20, 'H'), (21, 'I'), (22, 'J'), (23, 'K'), (24, 'L'), (25, 'M'), (26, 'N'), (27, 'O'), (28, 'P'), (29, 'Q'), (30, 'R'), (31, 'S'), (32, 'T'), (33, 'U'), (34, 'V'), (35, 'W'), (36, 'X'), (37, 'Y'), (38, 'Z'), (39, 'a'), (40, 'b'), (41, 'c'), (42, 'd'), (43, 'e'), (44, 'f'), (45, 'g'), (46, 'h'), (47, 'i'), (48, 'j'), (49, 'k'), (50, 'l'), (51, 'm'), (52, 'n'), (53, 'o'), (54, 'p'), (55, 'q'), (56, 'r'), (57, 's'), (58, 't'), (59, 'u'), (60, 'v'), (61, 'w'), (62, 'x'), (63, 'y'), (64, 'z')]\n",
      "65 Unique characters\n"
     ]
    }
   ],
   "source": [
    "# the unique characters in the file, meaning each character that is used \n",
    "vocab = sorted(set(text)) # return as a list\n",
    "print ([text for text in enumerate(vocab)])\n",
    "print(f'{len(vocab)} Unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It needs the text to be split into tokens first.\n",
    "# the 'b' prefix printed out indicates that these characters are stored as bytes objects.\n",
    "# When text data is read or processed in Python, especially with encoding methods like UTF-8, each character is represented as a byte in memory\n",
    "\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8') # contains a tensor where each element corresponds to a token (character) from the input texts provided in the example_texts list.\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create the tf.keras.layers.StringLookup layer:\n",
    "# This is NOT zero index. It starts from 1. and the IDs given are unique indices from the vocab variable. \"a\"'s index is 39 in vaocab as zero index\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary = list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invert back the vectors of IDs to Chars\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1) # chars_from_ids is an instance obj of tf.keras.layers.StringLookup, Joins all strings into a single string, or joins along an axis.\n",
    "\n",
    "# To make sure if it works\n",
    "# x = text_from_ids(ids)\n",
    "# print(\"x!!\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Examples and Targets\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs!!>>> tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "\"\"\"First prepare IDs, then use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices.\"\"\"\n",
    "# tf.strings.unicode_split makes each chars in text tokens\n",
    "# ids_from_chars variable has a class of tf.keras.layers.StringLookup in it which converts tokens into ids\n",
    "# print(\"tokens!!>>>\", tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "print(\"IDs!!>>>\",all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # ids_dataset becomes a class\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
    "\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convert these individual characters to sequences of the desired size\"\"\"\n",
    "# The batch method lets you easily do that.\n",
    "#  seq_length + 1, the model learns to predict the next character in the sequence given a fixed window of characters as input.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# It's easier to see what this is doing if you join the tokens back into strings:\n",
    "for seq in sequences.take(1):\n",
    "    print(text_from_ids(seq).numpy()) # text_from_ids is the custom function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. <br> At each time step the input is the current character and the label is the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Details below, but this function slides one char to right\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) #  map automatically takes an argument. dataset is an instance obj of the _MapDataset class\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches\n",
    "You used **tf.data** to split the text into manageable sequences. <br>\n",
    "But before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[47,  6, 43, ..., 64, 54, 60],\n",
      "       [26, 64,  2, ..., 58,  2, 58],\n",
      "       [51, 45,  2, ..., 40,  2, 51],\n",
      "       ...,\n",
      "       [32,  2, 37, ..., 53,  2, 40],\n",
      "       [60, 58, 48, ..., 45, 54, 57],\n",
      "       [45,  2, 14, ..., 41, 58, 44]]), array([[ 6, 43,  2, ..., 54, 60,  2],\n",
      "       [64,  2, 55, ...,  2, 58, 47],\n",
      "       [45,  2, 59, ...,  2, 51, 48],\n",
      "       ...,\n",
      "       [ 2, 37, 22, ...,  2, 40, 53],\n",
      "       [58, 48, 53, ..., 54, 57, 43],\n",
      "       [ 2, 14, 53, ..., 58, 44, 53]]))]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE) # This does Asynchronous data loading, i.e. improves the loading speed\n",
    ")\n",
    "# dataset\n",
    "print(list(dataset.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '[UNK]'), (1, '\\n'), (2, ' '), (3, '!'), (4, '$'), (5, '&'), (6, \"'\"), (7, ','), (8, '-'), (9, '.'), (10, '3'), (11, ':'), (12, ';'), (13, '?'), (14, 'A'), (15, 'B'), (16, 'C'), (17, 'D'), (18, 'E'), (19, 'F'), (20, 'G'), (21, 'H'), (22, 'I'), (23, 'J'), (24, 'K'), (25, 'L'), (26, 'M'), (27, 'N'), (28, 'O'), (29, 'P'), (30, 'Q'), (31, 'R'), (32, 'S'), (33, 'T'), (34, 'U'), (35, 'V'), (36, 'W'), (37, 'X'), (38, 'Y'), (39, 'Z'), (40, 'a'), (41, 'b'), (42, 'c'), (43, 'd'), (44, 'e'), (45, 'f'), (46, 'g'), (47, 'h'), (48, 'i'), (49, 'j'), (50, 'k'), (51, 'l'), (52, 'm'), (53, 'n'), (54, 'o'), (55, 'p'), (56, 'q'), (57, 'r'), (58, 's'), (59, 't'), (60, 'u'), (61, 'v'), (62, 'w'), (63, 'x'), (64, 'y'), (65, 'z')]\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in StringLookup layer\n",
    "vocab_size =len(ids_from_chars.get_vocabulary())\n",
    "print([x for x in enumerate(ids_from_chars.get_vocabulary())])\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This model has 3 layers <br>\n",
    "-tf.keras.layers.Embedding: The input layer. <br>\n",
    "-tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an **LSTM** layer here.) <br>\n",
    "-tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. <br>\n",
    "\n",
    "Make those as instance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    # The call method defines the forward pass of the model.\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training) \n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instant object\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Model\n",
    "Run the model to see that it behaves as expected.<br>\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, seq_length, vocab_size)\n",
      "Model: \"my_model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_4 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): # the len of input and target is both 64\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, seq_length, vocab_size)\")\n",
    "    \n",
    "    # the UNK token is added on vocab_size =len(ids_from_chars.get_vocabulary()) if you use Lookup layer, the size will automatically differ fron the original vocab (65) to vocab_size (66) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 21 33 28 11 53 38 51 50 38  0 31 27 55 40  6 52 51  3 54 65  3 57 36\n",
      " 53  4 28 42 18 40  8 60 62 45 39 43 33 36 52  4 58 27 65 18 38  2 50 62\n",
      " 65 11  3 50 32 28 26 12 19 22 37  2 14 16 24 53  1 12  1 12 45 64  7  9\n",
      " 51 53  1  4 24 54 24 43 12 52 18 17 23 15 35 24 21 47  7 58  3 37 59 37\n",
      " 39 15 55 24]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. \n",
    "This distribution is defined by the logits over the character vocabulary.\n",
    "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\"\"\"\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # 2D tensor\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # 1D tensor\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'own, his forehead, nay, the valley,\\nThe pretty dimples of his chin and cheek,\\nHis smiles,\\nThe very m'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"\\nHTO:nYlkY[UNK]RNpa'ml!oz!rWn$OcEa-uwfZdTWm$sNzEY kwz:!kSOM;FIX ACKn\\n;\\n;fy,.ln\\n$KoKd;mEDJBVKHh,s!XtXZBpK\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Decode these to see the text predicted by this untrained model:\"\"\"\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:         tf.Tensor(4.1887255, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "# print(\"y_true\", target_example_batch)\n",
    "# print(\"y_pred\", example_batch_predictions)\n",
    "# print(\"Prediction shape\", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.9387"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. \n",
    "To confirm this you can check that the exponential of the mean loss is approximately EQUAL to the vocabulary size. \n",
    "(A much higher loss means the model is sure of its wrong answers, and is badly initialized:)\"\"\"\n",
    "tf.exp(example_batch_mean_loss).numpy() # Exponential e = 2.718.. power to 4.189.. = 65.99708.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure checkpoints\n",
    "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directort where the checkpoints will be save\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 152s 872ms/step - loss: 2.7289\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 145s 836ms/step - loss: 1.9906\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 147s 848ms/step - loss: 1.7117\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, \n",
    "                                          states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "'Tis to thy Excarse.\n",
      "You how, but mastleman: ferved. And that with a'll,\n",
      "And then furm the kind\n",
      "And laddias of my gigethand badcis frown it\n",
      "Trwat hath burd the queen, is thee.\n",
      "Graye, depy it.\n",
      "\n",
      "GRETIO:\n",
      "Pray, for thize, sir, us Iw's Claitors, is thou,\n",
      "'Tiss for may what vains langer.\n",
      "\n",
      "CORIOLANUS:\n",
      "Be gord, he wark 'dill have doo:\n",
      "That will dutruchio's away to the deweren, prince, Pory.\n",
      "\n",
      "KATHARGIA:\n",
      "Nor oft with one pacess first as befter.\n",
      "\n",
      "QUEEN SARKE:\n",
      "No, not in to whem\n",
      "And mant thou dveet surst not hack that tomene me not pally is\n",
      "Beacke than thy royo! makt that your like!\n",
      "\n",
      "MENENCE:\n",
      "Whack not whyse wear'd fit\n",
      "with your worndry queen of you taulte my peace, 'scopes in thou when,\n",
      "Have mead spring abfuction be\n",
      "namer. Take your whore copes it ally\n",
      "Dose friends is shall bain, Wit, a forloway, efe, then, sood says as she nevies\n",
      "O' the curpaty,\n",
      "For not but that say on you. Thy house fould seed o' the pleecke what his lawy,\n",
      "And though meas you go; I faul gevat awst;\n",
      "'CComio! Ladt wain,\n",
      "Should t \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run Time: 1.3209328651428223\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun Time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save the model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"About tf.keras.layersStringLookup()\"\"\"\n",
    "\n",
    "# vocab1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# data = tf.constant([[\"a\", \"c\", \"d\"],[\"d\", \"z\", \"b\"]])\n",
    "# print(data)\n",
    "# layer = tf.keras.layers.StringLookup(vocabulary=vocab1)\n",
    "# layer(data)\n",
    "\n",
    "# vocab = ['apple', 'banana', 'orange']\n",
    "# string_lookup = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token='UNK') # UNK refers unknown and it gives the unknown element the index 0. StringLookup starts at 1 index\n",
    "\n",
    "# ids = string_lookup(['apple', 'kiwi', 'orange'])\n",
    "# print(ids.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Unused or test code below---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about batch in tf.data.Dataset'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"about batch in tf.data.Dataset\"\"\"\n",
    "# dataset = tf.data.Dataset.range(8)\n",
    "# dataset = dataset.batch(3)\n",
    "# print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "\n",
    "# dataset1 = tf.data.Dataset.range(8)\n",
    "# dataset1 = dataset1.batch(3, drop_remainder=True)\n",
    "# print(list(dataset1.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About take() in tf.data.Dataset'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About take() in tf.data.Dataset\"\"\"\n",
    "# dataset = tf.data.Dataset.range(10)\n",
    "# dataset = dataset.take(1)\n",
    "# list(dataset.as_numpy_iterator())\n",
    "\n",
    "# list(sequences.as_numpy_iterator())\n",
    "\n",
    "# seq = sequences.take(1)\n",
    "# list(seq.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How the split_input_target function works'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"How the split_input_target function works\"\"\"\n",
    "\n",
    "# sequence = [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "# sequence = list(\"Hello\") # equivalent above but more convenient\n",
    "\n",
    "# input, target = split_input_target(sequence)\n",
    "# print(input)\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About reduce_join'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About reduce_join\"\"\"\n",
    "# join = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']]).numpy()\n",
    "\n",
    "# join1 = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']], axis=-2).numpy() # axis=0 or -2 is column, axis=1 or -1 is row\n",
    "\n",
    "# join2 = tf.strings.reduce_join([['abc','123'],\n",
    "#                         ['def','456']],\n",
    "#                        axis=-1,\n",
    "#                        separator=\"<<<Tomb>>>\").numpy()\n",
    "\n",
    "# print(join)\n",
    "# print(join1)\n",
    "# print(join2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About tf.keras.losses.SparseCategoricalCrossentropy()\\nmeasures the difference between the true labels (y_true) and the predicted probabilities (y_pred). '"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "measures the difference between the true labels (y_true) and the predicted probabilities (y_pred). \"\"\"\n",
    "# y_true = [1, 2]\n",
    "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# # Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "# scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# scce(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"About float('inf')\""
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About float('inf')\"\"\"\n",
    "# <Set upper bound>\n",
    "# value = float(\"inf\") # this is the only way to print (value exceeds). Any number can not be greater than float(\"inf\") which is infinite\n",
    "# max_value = float(\"inf\")\n",
    "# # Check if a value exceeds the maximum allowed\n",
    "# if value >= max_value:\n",
    "#     print(\"Value exceeds the maximum allowed.\")\n",
    "# else:\n",
    "#     print(\"Yes\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About @tf.function'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"About @tf.function\"\"\"\n",
    "# official usage: tf.range(start, limit, delta=1, dtype=None, name='range')\n",
    "\n",
    "# start = 0\n",
    "# limit = 16\n",
    "# delta = 1 # this is like \"step\"\n",
    "# p = (tf.range(start, limit, delta))\n",
    "# p = tf.reshape(p, [4,4]).numpy()\n",
    "# print(p)\n",
    "\n",
    "\n",
    "# @tf.function  # The decorator converts `add` into a `PolymorphicFunction` internally but not human-eye-visible.\n",
    "# def add(a, b):\n",
    "#   return a + b\n",
    "\n",
    "# add(tf.ones([2, 2]), tf.ones([2, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
